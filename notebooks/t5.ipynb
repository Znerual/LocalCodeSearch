{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 25.0/25.0 [00:00<00:00, 9.92kB/s]\n",
      "Downloading: 100%|██████████| 1.21k/1.21k [00:00<00:00, 253kB/s]\n",
      "Downloading: 100%|██████████| 792k/792k [00:02<00:00, 296kB/s]  \n",
      "Downloading: 100%|██████████| 1.79k/1.79k [00:00<00:00, 653kB/s]\n",
      "/home/ruzickal/Code/Environments/Py310TF/lib/python3.10/site-packages/transformers/models/auto/modeling_auto.py:1062: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Downloading: 100%|██████████| 892M/892M [01:14<00:00, 12.0MB/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-base-finetuned-summarize-news\")\n",
    "\n",
    "def summarize(text, max_length=150):\n",
    "  input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length,  repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
    "\n",
    "  preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "\n",
    "  return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 11:59:22.918172: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 11:59:24.789289: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12/lib64:\n",
      "2023-02-16 11:59:24.789457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.8/lib64:/usr/local/cuda-11.7/lib64:/usr/local/cuda-11/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-12.0/lib64:/usr/local/cuda-12/lib64:\n",
      "2023-02-16 11:59:24.789469: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'a model that can be used for a general-purpose summarizer for academic and general usage. The model has been fine-tuned 11B google/flan-t5-xxl on several Summarization datasets. The results show that the model works well on lots of text, although trained with a max source length of 512 tokens and 150 max summary length.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"\"\"Multi-purpose Summarizer (Fine-tuned 11B google/flan-t5-xxl on several Summarization datasets)\n",
    "Open In Colab\n",
    "\n",
    "A fine-tuned version of google/flan-t5-xxl on various summarization datasets (xsum, wikihow, cnn_dailymail/3.0.0, samsum, scitldr/AIC, billsum, TLDR, wikipedia-summary)\n",
    "\n",
    "70% of the data was also filtered with the use of the contriever with a cosine similarity between text and summary of 0.6 as threshold.\n",
    "\n",
    "Goal: a model that can be used for a general-purpose summarizer for academic and general usage. Control over the type of summary can be given by varying the instruction prepended to the source document. The result works well on lots of text, although trained with a max source length of 512 tokens and 150 max summary length. \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fb44cecf14dfd8da4780477b24382cb97a73cb8f35185f19364e110880d8ec3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
